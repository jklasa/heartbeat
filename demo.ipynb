{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0620a74f-ccae-4beb-828d-0e0e0117e4cd",
   "metadata": {},
   "source": [
    "# Heartbeat\n",
    "---\n",
    "\n",
    "This project serves as a way to measure the \"heartbeat\" of the Internet. In this case, the stethoscope is this AI-enabled system, and we are measuring the state via Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38710ad-ce47-4c25-ac87-456ed185f699",
   "metadata": {},
   "source": [
    "The system runs in a local Kubernetes (K8s) cluster, but can conceivably be pushed to the cloud with ease. Since running a somewhat intensive K8s cluster is not an easy task, I will demonstrate the main mechanics of the project in this walkthrough and mock the Kubernetes services.\n",
    "\n",
    "Inside the K8s cluster is a Kafka service at the center of it all. This is also difficult to set up, so this will also be mocked here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23695486-70ff-4a70-b7bc-fb41cefe6ece",
   "metadata": {},
   "source": [
    "If running from Google Colab, run the command below to install the necessary Python dependencies to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d621036e-f99b-4432-8e72-a2ca077f8fcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (4.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (6.0)\n",
      "Requirement already satisfied: transformers[torch] in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (4.18.0)\n",
      "Requirement already satisfied: sty in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (1.0.4)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (0.12.1)\n",
      "Requirement already satisfied: filelock in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (1.22.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (2022.4.24)\n",
      "Requirement already satisfied: torch>=1.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from transformers[torch]) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[torch]) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "Requirement already satisfied: click in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from sacremoses->transformers[torch]) (8.1.3)\n",
      "Requirement already satisfied: six in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from sacremoses->transformers[torch]) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages (from sacremoses->transformers[torch]) (1.1.0)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy pyyaml \"transformers[torch]\" sty scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37085826-928b-4778-9a19-4fa62f04bd2a",
   "metadata": {},
   "source": [
    "# Mocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527c715-61d2-46e1-9298-10e035e8066d",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c2baa4a5-a8ed-475c-8ee1-35b61f34d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "from sty import fg, rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44809899-06e4-461c-97b8-ece9b2a36373",
   "metadata": {},
   "source": [
    "Kafka has a number of brokers that deal in messages based on topics. Producers and consumers can operate on these message streams by requesting or supplying data based on the desired topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4335f-c1a4-48f0-88c6-8652b9826eb9",
   "metadata": {},
   "source": [
    "Any data that enters Kafka is serialized by some method. In the real system, data is serialized by custom numeric serializers and an [Apache Avro](https://avro.apache.org/docs/current/spec.html) serializer. All tweet and sentiment data payloads sent via Kafka are stored as Avro data, which is a data serialization system for arbitrary data. To use it, I define schemas based on how the data is expected to present itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496d680-67a2-4399-bad1-09956a68607b",
   "metadata": {},
   "source": [
    "These schemas are registered with a Kafka SchemaRegistry service running in the K8s cluster. Here, I will just convert to and from normal dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "1e49fb75-c994-462a-bb0a-2bc7c5e086c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaStream:\n",
    "    def __init__(self):\n",
    "        print(fg.cyan + \"KAFKA \" + rs.all, end=\"\")\n",
    "        print(\"Connected to new Kafka broker\")\n",
    "        self.data = defaultdict(list)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def add(self, topic: str, key: int, value: dict) -> None:\n",
    "        self.lock.acquire()\n",
    "        print(fg.cyan + \"KAFKA \" + rs.all, end=\"\")\n",
    "        print(fg.magenta + f\"{topic} \" + rs.all, end=\"\")\n",
    "        print(fg.green + \"ADD\" + rs.all + f\" {key} = {value}\")\n",
    "        self.data[topic].append((key, value))\n",
    "        self.lock.release()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def get(self, topic) -> Tuple[int, str]:\n",
    "        self.lock.acquire()\n",
    "        print(fg.cyan + \"KAFKA \" + rs.all, end=\"\")\n",
    "        print(fg.magenta + f\"{topic} \" + rs.all, end=\"\")\n",
    "        if len(self.data[topic]) > 0:\n",
    "            res = self.data[topic].pop(0)\n",
    "            print(fg.red + \"GET\" + rs.all + f\" {res[0]} = {res[1]}\")\n",
    "        else:\n",
    "            res = None\n",
    "            print(fg.red + \"GET\" + rs.all + \" EMPTY\")\n",
    "        self.lock.release()\n",
    "        time.sleep(0.1)\n",
    "        return res "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28db28-5d27-47d6-9d7b-b36e14a702a5",
   "metadata": {},
   "source": [
    "Producers, as their name would suggest, produce data to given topics in Kafka. Consumers pull the data out of Kafka as it is produced by the producers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "3739677d-e422-4be2-90dc-7c68ed878fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaProducer:\n",
    "    def __init__(self, stream: KafkaStream, serialize: Callable):\n",
    "        self.stream = stream\n",
    "        self.serialize = serialize\n",
    "        self.buffer = []\n",
    "    \n",
    "    def produce(self, topic, key, value) -> None:\n",
    "        self.buffer.append((topic, key, self.serialize(value, None)))\n",
    "        \n",
    "    def poll(self) -> int:\n",
    "        size = len(self.buffer)\n",
    "        for item in self.buffer:\n",
    "            self.stream.add(*item)\n",
    "        self.buffer = []\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "4c97ba74-9a16-49df-a1d2-4af746e8517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaConsumer:\n",
    "    def __init__(self, stream: KafkaStream, deserialize: Callable):\n",
    "        self.stream = stream\n",
    "        self.deserialize = deserialize\n",
    "        self.topic = None\n",
    "        \n",
    "    def subscribe(self, topic) -> None:\n",
    "        self.topic = topic\n",
    "    \n",
    "    def poll(self) -> Any:\n",
    "        res = self.stream.get(self.topic)\n",
    "        if res is None:\n",
    "            return None\n",
    "        key, value = res\n",
    "        return key, self.deserialize(value, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014903c2-148e-4005-a975-f087d5314f81",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209bcc24-4945-43b9-90f5-9b5fc7ff2878",
   "metadata": {},
   "source": [
    "Here is a client that will produce random fake Tweet data for this demonstration. Access to the actual API requires authorization dependent on the user. I have an account with [Elevated](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level) access that I personally requested from Twitter, so I figure it would not be a good idea to publicly post the tokens on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e17cc050-808b-47a8-aa3e-b8d9a69aa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3209d286-f368-47c0-9fc4-9ff922f2ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TweepyTweet:\n",
    "    id: int\n",
    "    text: str\n",
    "    created_at: datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8a649559-a179-4018-b611-88b110e81409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweepyClient(ABC):\n",
    "    def __init__(self):\n",
    "        word_site = \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "        response = requests.get(word_site)\n",
    "        self.vocab = [b.decode(\"utf-8\") for b in response.content.splitlines()]\n",
    "\n",
    "    def start(self, num: int = 100):\n",
    "        return Thread(target=self.generate, args=(num,)).start()\n",
    "\n",
    "    def generate(self, num: int):\n",
    "        for i in range(num):\n",
    "            tweet = TweepyTweet(\n",
    "                id=random.randint(0, 10000),\n",
    "                text=\" \".join(random.sample(self.vocab, 15)),\n",
    "                created_at=datetime.utcnow(),\n",
    "            )\n",
    "            self.on_tweet(tweet)\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_tweet(self, tweet):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e4ebc-dd7f-4eab-b0b9-3e3ecf62edc0",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f357669-4d88-4f86-9d09-1974083e14de",
   "metadata": {},
   "source": [
    "This code here is similar to what I was actually writing. Here is an abstract data type used to define some form of data being stored in Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "06c9114e-6a68-4f64-b6b7-a7eefa5961b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ADT(ABC):\n",
    "    def to_dict(self, ctx):\n",
    "        return self.__dict__\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, obj, ctx):\n",
    "        if obj is None:\n",
    "            return None\n",
    "        return cls(**obj)\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def schema(cls) -> str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "62725522-c149-44b1-803c-e3e664611464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Producer(KafkaProducer):\n",
    "    def __init__(self, topic: str, data: ADT, stream: KafkaStream):\n",
    "        self.topic = topic\n",
    "        super().__init__(stream, data.to_dict)\n",
    "        \n",
    "    def produce(self, key: int, value: Any) -> None:\n",
    "        super().produce(self.topic, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c0a0cffe-f836-411d-b14f-b64812054217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Consumer(KafkaConsumer):\n",
    "    def __init__(self, data: ADT, stream: KafkaStream):\n",
    "        super().__init__(stream, data.from_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549b517-bcce-4ef9-918b-3d4508cde41c",
   "metadata": {},
   "source": [
    "# Twitter Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "644fabc9-1f67-434d-a54a-b1afeccd6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979d271-1c65-4f3d-9b75-513484ad61b7",
   "metadata": {},
   "source": [
    "The first step in the system is to ingest content from Twitter. In this case, data comes from the Twitter filtered stream endpoint via `POST /2/tweets/search/stream`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f803bf-7e09-4c13-926f-d83dc7eac587",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e22fa-0878-4fd3-b8f2-109358d816aa",
   "metadata": {},
   "source": [
    "Data is represented using one of my abstract data types. I also define a schema for the Kafka SchemaRegistry service so that it can properly understand the incoming and outgoing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd5f32-1d6c-4879-b365-15f3e7ac1355",
   "metadata": {},
   "source": [
    "Twitter data only needs 3 attributes for representing Tweets:\n",
    "\n",
    "* **task** - the Heartbeat tasking. This is the subject of the search. In our case, I was searching for Russia-Ukraine information.\n",
    "* **content** - the text content of the Tweet.\n",
    "* **time** - timestamp with only second precision. High precision is not really needed for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ac283bf0-1fa1-4a37-9e33-78d9b08bb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(ADT):\n",
    "    def __init__(self, task: str, content: str, time: int):\n",
    "        self.task = task\n",
    "        self.content = content\n",
    "        self.time = time\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def schema(cls) -> str:\n",
    "        return \"\"\"\n",
    "        {\n",
    "            \"name\": \"tweet\",\n",
    "            \"type\": \"record\",\n",
    "            \"namespace\": \"heartbeat\",\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"time\",\n",
    "                    \"type\": {\n",
    "                        \"type\": \"int\", \n",
    "                        \"logicalType\": \"timestamp-millis\"\n",
    "                    }\n",
    "                },\n",
    "                {\"name\": \"content\", \"type\": \"string\"},\n",
    "                {\"name\": \"task\", \"type\": \"string\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc03d3-e68d-4109-a78e-055b5f1c8d72",
   "metadata": {},
   "source": [
    "## Tweepy - Kafka Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d13ce0-1314-4598-a5d4-d33eeb941aa6",
   "metadata": {},
   "source": [
    "Withe some of the mocking and setup out of the way, we can look at the services. The first part of the Heartbeat system involves ingesting the data from Twitter and producing it to the \"ingest\" topic in Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350865a-dc78-4cfb-8618-860a346f6edb",
   "metadata": {},
   "source": [
    "This simple service just connects to the Twitter stream endpoint, and as Tweets are received, pushes structured data to the topic. Data comes in from a Python wrapper for the Twitter API called `Tweepy`. This library allowed for very simple access to the Twitter API. The work here was just a matter of connecting the stream of data coming from Twitter and Tweepy to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "0862db48-55f4-4847-9c6d-8d0d92fffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIngest(TweepyClient):\n",
    "    def __init__(self, task: str, producer: Producer):\n",
    "        self.task = task\n",
    "        self.producer = producer\n",
    "        super().__init__()\n",
    "\n",
    "    def on_tweet(self, tweet):\n",
    "        payload = Tweet(self.task, tweet.text, tweet.created_at.timestamp())\n",
    "        self.producer.produce(key=tweet.id, value=payload)\n",
    "\n",
    "    def poll(self) -> None:\n",
    "        empty = False\n",
    "        while not empty:\n",
    "            time.sleep(0.3)\n",
    "            empty = self.producer.poll() == 0\n",
    "        print(\">>> Exiting Twitter ingest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2155b-c960-4ef2-bd45-80759d8eafaa",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "0f7d6b1a-abc5-43fe-857e-67afd3083c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mKAFKA \u001b[0mConnected to new Kafka broker\n"
     ]
    }
   ],
   "source": [
    "stream = KafkaStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "8fcebf05-6215-4e8d-b251-6303ca9534de",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = Producer(\"ingest\", Tweet, stream)\n",
    "ingest = TwitterIngest(\"RU-UKR\", producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "68068544-233c-424f-b3dc-afc57e2d0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 7868 = {'task': 'RU-UKR', 'content': 'slovakia vibrators acrobat boy each reconstruction leaf bugs operated health army rendering audio bath continues', 'time': 1652072799.379839}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 7489 = {'task': 'RU-UKR', 'content': 'oldest copper romantic thats defense largely pas qty perceived uses playlist totally human tapes keyboard', 'time': 1652072799.379868}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 8968 = {'task': 'RU-UKR', 'content': 'dried pill expiration attached symbol estimated past instrumental conservation tickets litigation souls docs penetration outstanding', 'time': 1652072799.379883}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 5836 = {'task': 'RU-UKR', 'content': 'slideshow add associates decade learned bonds kruger hollow vinyl conditional flashers encryption instantly awards legislature', 'time': 1652072799.379897}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 2722 = {'task': 'RU-UKR', 'content': 'berry ftp playboy somewhere caution defects write bell questions through antigua phoenix gale feb enjoying', 'time': 1652072799.37991}\n",
      ">>> Exiting Twitter ingest\n"
     ]
    }
   ],
   "source": [
    "t = ingest.start(5)\n",
    "ingest.poll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8b6fc-2341-4f15-bccc-a4e26d6427d7",
   "metadata": {},
   "source": [
    "So now we have an incoming stream of Tweets with a very simple data schema. Next, we just have to process this data for sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32759110-ed9b-4b21-b739-b3cddb906361",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f70f4-f8bc-4a7d-82ed-1714c8d92b1a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d428e7a-2aff-4a19-9d43-7406fda63b07",
   "metadata": {},
   "source": [
    "As before, there will be some more data structures to deal with. This time, we will be using data storage for sentiment analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4faed-8f36-452d-9227-073f682a3de7",
   "metadata": {},
   "source": [
    "This time, we are still storing the task information, but we are now also storing 3 different floating-point values for Tweet sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb214f8-282a-417f-a301-54fcd72b26ba",
   "metadata": {},
   "source": [
    "* **task** - the Heartbeat tasking. This is the subject of the search. In our case, I was searching for Russia-Ukraine information.\n",
    "* **time** - timestamp with only second precision. High precision is not really needed for this purpose. Should still be the timestamp from Twitter, not one that we create.\n",
    "* **pos** - likelihood of positive sentiment. Values range from 0 to 1 such that higher values indicate higher likelihood. Positive sentiment indicates favorable opinion of the tasking.\n",
    "* **neu** - likelihood of neutral sentiment. Neutral sentiment indicates no particular positive or negative opinion.\n",
    "* **neg** - likelihood of negative sentiment. Negative sentiment indicates some degree of dislike with the tasking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "c9fb7851-db24-4204-95d1-bbc52163fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(ADT):\n",
    "    def __init__(\n",
    "        self, task: str, time: int, pos: float, neu: float, neg: float\n",
    "    ):\n",
    "        self.task = task\n",
    "        self.time = time\n",
    "        self.pos = pos\n",
    "        self.neu = neu\n",
    "        self.neg = neg\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def schema(cls) -> str:\n",
    "        return \"\"\"\n",
    "        {\n",
    "            \"name\": \"sentiment\",\n",
    "            \"type\": \"record\",\n",
    "            \"namespace\": \"heartbeat\",\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"time\",\n",
    "                    \"type\": {\"type\": \"int\", \"logicalType\": \"timestamp-millis\"}\n",
    "                },\n",
    "                {\"name\": \"task\", \"type\": \"string\"},\n",
    "                {\"name\": \"pos\", \"type\": \"float\"},\n",
    "                {\"name\": \"neu\", \"type\": \"float\"},\n",
    "                {\"name\": \"neg\", \"type\": \"float\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732c924-d77e-4dd0-a031-bd2d4fa29f6c",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "109f2cbc-69dc-4dee-b4d3-ba2730655935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "\n",
    "    def score(self, text: str) -> List[float]:\n",
    "        processed = SentimentAnalyzer.preprocess(text)\n",
    "        encoded_input = self.tokenizer(processed, return_tensors=\"pt\")\n",
    "        output = self.model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        return softmax(scores) # negative, neutral, positive\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> str:\n",
    "        # Preprocess text (username and link placeholders)\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = \"@user\" if t.startswith(\"@\") and len(t) > 1 else t\n",
    "            t = \"http\" if t.startswith(\"http\") else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "fa948757-817a-495d-8019-e334893ea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(stream, analyzer):\n",
    "    consumer = Consumer(Tweet, stream)\n",
    "    consumer.subscribe(\"ingest\")\n",
    "    producer = Producer(\"sentiment\", Sentiment, stream)\n",
    "\n",
    "    empty = 0\n",
    "    while empty < 3:\n",
    "        msg = consumer.poll()\n",
    "        if msg is None:\n",
    "            empty += 1\n",
    "            continue\n",
    "\n",
    "        key, tweet = msg\n",
    "        if tweet is not None:\n",
    "            negative, neutral, positive = analyzer.score(tweet.content)\n",
    "            sentiment = Sentiment(\n",
    "                task=tweet.task,\n",
    "                time=tweet.time,\n",
    "                pos=positive,\n",
    "                neu=neutral,\n",
    "                neg=negative,\n",
    "            )\n",
    "            producer.produce(key=key, value=sentiment)\n",
    "        producer.poll()\n",
    "    print(\">>> Exiting sentiment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d393bb-389b-4b2b-842e-bd97291451d8",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee2f98-ff29-4f22-8eaf-50eafff238cc",
   "metadata": {},
   "source": [
    "Next, we just have to load the model from [HuggingFace](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest), where pretrained models are made extremely accessible for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d1d8d465-346e-4aa8-b054-7b718fc9cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set up sentiment analysis\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22101fc3-7352-49b5-a220-1bd067549fca",
   "metadata": {},
   "source": [
    "With the models loaded, we can determine sentiment from the text. Note that the return order from the model is [negative, neutral, positive]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "cb302259-d23d-40f6-92e8-eca47cd7a225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00420294, 0.00935337, 0.98644376], dtype=float32)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.score(\"We are absolutely loving this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "13e3db83-b29e-47d1-b8a9-0f7d4714ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8545521 , 0.11869626, 0.02675165], dtype=float32)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.score(\"This is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "93417e7c-34ce-4d17-8d4a-70c5b5d3a600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02685222, 0.7657344 , 0.20741342], dtype=float32)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.score(\"HuggingFace is a machine learning library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e61c1e-dc14-4bc5-820a-8d34630d96f0",
   "metadata": {},
   "source": [
    "We can even attach this analyzer to the Tweet data. The text is random gibberish, so the sentiment outputs are not very useful, but the system can still be demonstrated in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "cf6b5dd6-c979-4ca7-8760-21fd926e1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoAnalyzer(TweepyClient):\n",
    "    def __init__(self, analyzer, *args, **kwargs):\n",
    "        self.analyzer = analyzer\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def on_tweet(self, tweet):\n",
    "        print(f\"> TEXT: {tweet.text}\")\n",
    "        pos, neu, neg = self.analyzer.score(tweet.text)\n",
    "        print(f\"  SENTIMENT: pos={pos:.3f}, neu={neu:.3f}, neg={neg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "b325b042-9c03-43b6-92fb-61ddfa858ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DemoAnalyzer(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "15e11bfb-4348-40d1-b88c-813eace8a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> TEXT: breath richard peru compiled sacramento quiet coding cache identified john pd being automotive sisters mountain\n",
      "  SENTIMENT: pos=0.030, neu=0.900, neg=0.070\n",
      "> TEXT: neural special broker cio leading collect handled audi widescreen newspapers representative em buried flower ri\n",
      "  SENTIMENT: pos=0.025, neu=0.928, neg=0.047\n",
      "> TEXT: mattress gays sea rss whether interests describes logs recovered beverage reviews fr support acceptable be\n",
      "  SENTIMENT: pos=0.041, neu=0.920, neg=0.040\n",
      "> TEXT: properties thereafter harper wicked colour chile weekends liberal liabilities rank bukkake seller commodities belize portal\n",
      "  SENTIMENT: pos=0.637, neu=0.347, neg=0.016\n",
      "> TEXT: proper sacred shirts highs nasdaq buzz adrian contrast gd cart around preventing player not blogging\n",
      "  SENTIMENT: pos=0.105, neu=0.781, neg=0.114\n"
     ]
    }
   ],
   "source": [
    "da.start(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f363f1b-0d42-4ed6-b921-5d646ad220e5",
   "metadata": {},
   "source": [
    "# Database Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277d154-96de-486a-ae16-71f72b5bc70a",
   "metadata": {},
   "source": [
    "The last step is transfer of the results from Kafka to a database for storage. For this project, I used [InfluxDB](https://github.com/influxdata/influxdb), an efficient time-series database for storing the sentiments according to the timestamps at which they were retrieved. This will again be mocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "ca507c79-0740-4774-b100-11b9d99b1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self, lock):\n",
    "        self.data = []\n",
    "        self.lock = lock\n",
    "        \n",
    "    def write(self, item):\n",
    "        self.lock.acquire()\n",
    "        print(fg.grey + \"DB \" + rs.all, end=\"\")\n",
    "        print(fg.green + \"ADD\" + rs.all + f\" {item}\")\n",
    "        \n",
    "        self.lock.release()\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def get_all(self):\n",
    "        print(\"Database dump...\")\n",
    "        for idx, item in enumerate(self.data):\n",
    "            print(f\"{idx}) {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "507ba7ac-7363-42dc-9efd-f8dbd1cf5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(stream: KafkaStream, db: Database):\n",
    "    consumer = Consumer(Sentiment, stream)\n",
    "    consumer.subscribe(\"sentiment\")\n",
    "\n",
    "    empty = 0\n",
    "    while empty < 10:\n",
    "        msg = consumer.poll()\n",
    "        if msg is None:\n",
    "            empty += 1\n",
    "            continue\n",
    "\n",
    "        key, sent = msg\n",
    "        db.write([key, sent.task, sent.time, sent.pos, sent.neu, sent.neg])\n",
    "    print(\">>> Exiting data storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa83566-1b80-475f-98d9-35451ad3eecb",
   "metadata": {},
   "source": [
    "# Full Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9ff13-ce61-4cc5-a09c-403a8b423210",
   "metadata": {},
   "source": [
    "Now let's put it all together and let the ingest service retrieve data from \"Twitter\" and push it to \"Kafka\". Meanwhile, we will let the analyzer service retrieve data from \"Kafka\" and produce sentiment analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "85a91d5b-98eb-43cb-9896-685118a857bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mKAFKA \u001b[0mConnected to new Kafka broker\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 1783 = {'task': 'RU-UKR', 'content': 'catalogs frozen creatures pontiac fast quarters cite earn saskatchewan performing manually thats closed meet robinson', 'time': 1652073462.491736}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 9088 = {'task': 'RU-UKR', 'content': 'skilled going lie anonymous military rehab convinced heavy mw indonesian top entered museums sixth sensitive', 'time': 1652073462.491762}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 3479 = {'task': 'RU-UKR', 'content': 'piss casual buyers rise soap fa audit ranks monica association buried annotation sao buy killed', 'time': 1652073462.491772}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 3815 = {'task': 'RU-UKR', 'content': 'baths dan precise pot independent difficulty gear lower transit apparently chicago spider mesa crm antiques', 'time': 1652073462.491781}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 289 = {'task': 'RU-UKR', 'content': 'floors april trial farms fast aim formerly stability witnesses solution crime exam persons phone cz', 'time': 1652073462.49179}\n",
      ">>> Exiting Twitter ingest\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 1783 = {'task': 'RU-UKR', 'content': 'catalogs frozen creatures pontiac fast quarters cite earn saskatchewan performing manually thats closed meet robinson', 'time': 1652073462.491736}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "{'task': 'RU-UKR', 'content': 'catalogs frozen creatures pontiac fast quarters cite earn saskatchewan performing manually thats closed meet robinson', 'time': 1652073462.491736}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 1783 = {'task': 'RU-UKR', 'time': 1652073462.491736, 'pos': 0.023776477, 'neu': 0.82779956, 'neg': 0.14842393}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 1783 = {'task': 'RU-UKR', 'time': 1652073462.491736, 'pos': 0.023776477, 'neu': 0.82779956, 'neg': 0.14842393}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 9088 = {'task': 'RU-UKR', 'content': 'skilled going lie anonymous military rehab convinced heavy mw indonesian top entered museums sixth sensitive', 'time': 1652073462.491762}\n",
      "{'task': 'RU-UKR', 'time': 1652073462.491736, 'pos': 0.023776477, 'neu': 0.82779956, 'neg': 0.14842393}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [1783, 'RU-UKR', 1652073462.491736, 0.023776477, 0.82779956, 0.14842393]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "{'task': 'RU-UKR', 'content': 'skilled going lie anonymous military rehab convinced heavy mw indonesian top entered museums sixth sensitive', 'time': 1652073462.491762}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 9088 = {'task': 'RU-UKR', 'time': 1652073462.491762, 'pos': 0.03266897, 'neu': 0.6172506, 'neg': 0.35008043}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 9088 = {'task': 'RU-UKR', 'time': 1652073462.491762, 'pos': 0.03266897, 'neu': 0.6172506, 'neg': 0.35008043}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 3479 = {'task': 'RU-UKR', 'content': 'piss casual buyers rise soap fa audit ranks monica association buried annotation sao buy killed', 'time': 1652073462.491772}\n",
      "{'task': 'RU-UKR', 'time': 1652073462.491762, 'pos': 0.03266897, 'neu': 0.6172506, 'neg': 0.35008043}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [9088, 'RU-UKR', 1652073462.491762, 0.03266897, 0.6172506, 0.35008043]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "{'task': 'RU-UKR', 'content': 'piss casual buyers rise soap fa audit ranks monica association buried annotation sao buy killed', 'time': 1652073462.491772}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 3479 = {'task': 'RU-UKR', 'time': 1652073462.491772, 'pos': 0.008746965, 'neu': 0.12436981, 'neg': 0.8668834}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 3479 = {'task': 'RU-UKR', 'time': 1652073462.491772, 'pos': 0.008746965, 'neu': 0.12436981, 'neg': 0.8668834}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 3815 = {'task': 'RU-UKR', 'content': 'baths dan precise pot independent difficulty gear lower transit apparently chicago spider mesa crm antiques', 'time': 1652073462.491781}\n",
      "{'task': 'RU-UKR', 'time': 1652073462.491772, 'pos': 0.008746965, 'neu': 0.12436981, 'neg': 0.8668834}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [3479, 'RU-UKR', 1652073462.491772, 0.008746965, 0.12436981, 0.8668834]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "{'task': 'RU-UKR', 'content': 'baths dan precise pot independent difficulty gear lower transit apparently chicago spider mesa crm antiques', 'time': 1652073462.491781}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 3815 = {'task': 'RU-UKR', 'time': 1652073462.491781, 'pos': 0.054548632, 'neu': 0.9132545, 'neg': 0.032196872}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 3815 = {'task': 'RU-UKR', 'time': 1652073462.491781, 'pos': 0.054548632, 'neu': 0.9132545, 'neg': 0.032196872}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 289 = {'task': 'RU-UKR', 'content': 'floors april trial farms fast aim formerly stability witnesses solution crime exam persons phone cz', 'time': 1652073462.49179}\n",
      "{'task': 'RU-UKR', 'time': 1652073462.491781, 'pos': 0.054548632, 'neu': 0.9132545, 'neg': 0.032196872}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [3815, 'RU-UKR', 1652073462.491781, 0.054548632, 0.9132545, 0.032196872]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "{'task': 'RU-UKR', 'content': 'floors april trial farms fast aim formerly stability witnesses solution crime exam persons phone cz', 'time': 1652073462.49179}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 289 = {'task': 'RU-UKR', 'time': 1652073462.49179, 'pos': 0.105619214, 'neu': 0.86781275, 'neg': 0.026568046}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 289 = {'task': 'RU-UKR', 'time': 1652073462.49179, 'pos': 0.105619214, 'neu': 0.86781275, 'neg': 0.026568046}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "{'task': 'RU-UKR', 'time': 1652073462.49179, 'pos': 0.105619214, 'neu': 0.86781275, 'neg': 0.026568046}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [289, 'RU-UKR', 1652073462.49179, 0.105619214, 0.86781275, 0.026568046]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      ">>> Exiting data storage\n",
      ">>> Exiting sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "stream = KafkaStream()\n",
    "db = Database(stream.lock)\n",
    "producer = Producer(\"ingest\", Tweet, stream)\n",
    "ingest = TwitterIngest(\"RU-UKR\", producer)\n",
    "\n",
    "t = ingest.start(5)\n",
    "Thread(target=ingest.poll()).start()\n",
    "Thread(target=analyze, args=(stream, analyzer)).start()\n",
    "Thread(target=transfer, args=(stream, db)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff55229-1937-4cf3-940c-c6e12f0b241d",
   "metadata": {},
   "source": [
    "And then, of course, we can get the results in the database and easily inspect them. InfluxDB makes this very easy as well, as it has built-in dashboards for visualization of time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "eaea306e-62e2-4887-92b5-293fb13534cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database dump...\n",
      "0) [1783, 'RU-UKR', 1652073462.491736, 0.023776477, 0.82779956, 0.14842393]\n",
      "1) [9088, 'RU-UKR', 1652073462.491762, 0.03266897, 0.6172506, 0.35008043]\n",
      "2) [3479, 'RU-UKR', 1652073462.491772, 0.008746965, 0.12436981, 0.8668834]\n",
      "3) [3815, 'RU-UKR', 1652073462.491781, 0.054548632, 0.9132545, 0.032196872]\n",
      "4) [289, 'RU-UKR', 1652073462.49179, 0.105619214, 0.86781275, 0.026568046]\n"
     ]
    }
   ],
   "source": [
    "db.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73381f-ad3b-419c-952d-18543a0aab86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hbtest]",
   "language": "python",
   "name": "conda-env-hbtest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
