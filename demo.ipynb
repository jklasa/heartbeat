{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0620a74f-ccae-4beb-828d-0e0e0117e4cd",
   "metadata": {},
   "source": [
    "# Heartbeat\n",
    "---\n",
    "\n",
    "This project serves as a way to measure the \"heartbeat\" of the Internet. In this case, the stethoscope is this AI-enabled system, and we are measuring the state via Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38710ad-ce47-4c25-ac87-456ed185f699",
   "metadata": {},
   "source": [
    "The system runs in a local Kubernetes (K8s) cluster, but can conceivably be pushed to the cloud with ease. Since running a somewhat intensive K8s cluster is not an easy task, I will demonstrate the main mechanics of the project in this walkthrough and mock the Kubernetes services.\n",
    "\n",
    "Inside the K8s cluster is a Kafka service at the center of it all. This is also difficult to set up, so this will also be mocked here. I use mocking in this case to mean the environment will be simulated - the actual functions will not be mocked, but the behavior will be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad577ae-1750-4488-ae0f-42575e07981c",
   "metadata": {},
   "source": [
    "There are 3 main processes happening in this system, and we will cover all 3 in simple demonstrations in this notebook.\n",
    "\n",
    "1. **Twitter ingest** - Retrieve data from Twitter via a filtered stream and push each Tweet to Kafka. The filter is based on dynamic tasking: the system accepts search rules that can be used to filter Tweets and assign topic tags.\n",
    "2. **Sentiment analysis** - Retrieve Tweets from Kafka, run them through a sentiment analysis model, and push them back to Kafka with their sentiment results.\n",
    "3. **Database storage** - Retrieve Tweets and their sentiments from Kafka and push them to final storage in time series database for aggregation and analysis.\n",
    "\n",
    "Each of these services has an associated Docker image and deployment running in Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527c715-61d2-46e1-9298-10e035e8066d",
   "metadata": {},
   "source": [
    "# Mocking Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44809899-06e4-461c-97b8-ece9b2a36373",
   "metadata": {},
   "source": [
    "Kafka has a number of brokers that deal in messages based on topics. Producers and consumers can operate on these message streams by requesting or supplying data based on the desired topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4335f-c1a4-48f0-88c6-8652b9826eb9",
   "metadata": {},
   "source": [
    "Any data that enters Kafka is serialized by some method. In the real system, data is serialized by custom numeric serializers and an [Apache Avro](https://avro.apache.org/docs/current/spec.html) serializer. All tweet and sentiment data payloads sent via Kafka are stored as Avro data, which is a data serialization system for arbitrary data. To use it, I define schemas based on how the data is expected to present itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496d680-67a2-4399-bad1-09956a68607b",
   "metadata": {},
   "source": [
    "These schemas are registered with a Kafka SchemaRegistry service running in the K8s cluster. Here, I will just convert to and from normal dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff99951-20fb-4fff-b313-b9bdf6239aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sty\n",
      "  Using cached sty-1.0.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: sty\n",
      "Successfully installed sty-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install sty \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2baa4a5-a8ed-475c-8ee1-35b61f34d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "from sty import fg, rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ed04b-c802-43bb-a7cc-c3003e3af8f3",
   "metadata": {},
   "source": [
    "Here, we start making the simulated environment for Kafka. This `KafkaStream` below represents a Kafka broker, where we can add (key, value) pairs of data to arbitrary topics and pull (key, value) pairs from those same topics as they are produced. The asynchronous behavior will be simulated with Python's threading module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e49fb75-c994-462a-bb0a-2bc7c5e086c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaStream:\n",
    "    def __init__(self):\n",
    "        print(fg.cyan + \"KAFKA \" + rs.all, end=\"\")\n",
    "        print(\"Connected to new Kafka broker\")\n",
    "        self.data = defaultdict(list)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # Add data to a topic\n",
    "    def add(self, topic: str, key: int, value: dict) -> None:\n",
    "        self.lock.acquire()\n",
    "        print(fg.cyan + \"KAFKA \" + rs.all, end=\"\")\n",
    "        print(fg.magenta + f\"{topic} \" + rs.all, end=\"\")\n",
    "        print(fg.green + \"ADD\" + rs.all + f\" {key} = {value}\")\n",
    "        self.data[topic].append((key, value))\n",
    "        self.lock.release()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # Get data from a topic, if there is any\n",
    "    def get(self, topic) -> Tuple[int, str]:\n",
    "        self.lock.acquire()\n",
    "        print(fg.cyan + \"KAFKA \" + rs.all, end=\"\")\n",
    "        print(fg.magenta + f\"{topic} \" + rs.all, end=\"\")\n",
    "        if len(self.data[topic]) > 0:\n",
    "            res = self.data[topic].pop(0)\n",
    "            print(fg.red + \"GET\" + rs.all + f\" {res[0]} = {res[1]}\")\n",
    "        else:\n",
    "            res = None\n",
    "            print(fg.red + \"GET\" + rs.all + \" EMPTY\")\n",
    "        self.lock.release()\n",
    "        time.sleep(0.1)\n",
    "        return res "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28db28-5d27-47d6-9d7b-b36e14a702a5",
   "metadata": {},
   "source": [
    "Producers, as their name would suggest, produce data to given topics in Kafka. Consumers pull the data out of Kafka as it is produced by the producers. In the simulated environment and in the actual production environment, these producers and consumers interact with the Kafka broker through production and polling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f87bc4-6be7-49e4-97e7-ce505997910c",
   "metadata": {},
   "source": [
    "The interfaces shown below closely mirror their actual usage in the production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3739677d-e422-4be2-90dc-7c68ed878fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaProducer:\n",
    "    def __init__(self, stream: KafkaStream, serialize: Callable):\n",
    "        self.stream = stream\n",
    "        self.serialize = serialize\n",
    "        self.buffer = []\n",
    "    \n",
    "    def produce(self, topic, key, value) -> None:\n",
    "        # Messages are buffered for more accurate simulation.\n",
    "        # Use poll() to flush the buffer.\n",
    "        self.buffer.append((topic, key, self.serialize(value, None)))\n",
    "        \n",
    "    def poll(self) -> int:\n",
    "        size = len(self.buffer)\n",
    "        for item in self.buffer:\n",
    "            self.stream.add(*item)\n",
    "        self.buffer = []\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c97ba74-9a16-49df-a1d2-4af746e8517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KafkaConsumer:\n",
    "    def __init__(self, stream: KafkaStream, deserialize: Callable):\n",
    "        self.stream = stream\n",
    "        self.deserialize = deserialize\n",
    "        self.topic = None\n",
    "        \n",
    "    def subscribe(self, topic) -> None:\n",
    "        self.topic = topic\n",
    "    \n",
    "    def poll(self) -> Any:\n",
    "        res = self.stream.get(self.topic)\n",
    "        if res is None:\n",
    "            return None\n",
    "        key, value = res\n",
    "        return key, self.deserialize(value, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e4ebc-dd7f-4eab-b0b9-3e3ecf62edc0",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e87ec-49d1-457c-b59f-82beaf086c69",
   "metadata": {},
   "source": [
    "Here begins the design framework that I built for working with Kafka messages and my unique data needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f357669-4d88-4f86-9d09-1974083e14de",
   "metadata": {},
   "source": [
    "Below is an abstract data type used to define some form of data used in the Heartbeat platform. This allowed me to very easily define data types that would be stored or moved in the form of Kafka messages. As mentioned before, I used Kafka in an entirely serialized fashion, so there had to be efficient methods for moving to and from serialized bytes and usable Python datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c9114e-6a68-4f64-b6b7-a7eefa5961b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ADT(ABC):\n",
    "    def to_dict(self, ctx):\n",
    "        return self.__dict__\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, obj, ctx):\n",
    "        if obj is None:\n",
    "            return None\n",
    "        return cls(**obj)\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def schema(cls) -> str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784d6be-99b9-402e-b523-9b322a154a11",
   "metadata": {},
   "source": [
    "I built my own custom Producer and Consumers on top of the library SerializedProducer and SerializedConsumer classes for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62725522-c149-44b1-803c-e3e664611464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Producer(KafkaProducer):\n",
    "    def __init__(self, topic: str, data: ADT, stream: KafkaStream):\n",
    "        self.topic = topic\n",
    "        super().__init__(stream, data.to_dict)\n",
    "        \n",
    "    def produce(self, key: int, value: Any) -> None:\n",
    "        super().produce(self.topic, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a0cffe-f836-411d-b14f-b64812054217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Consumer(KafkaConsumer):\n",
    "    def __init__(self, data: ADT, stream: KafkaStream):\n",
    "        super().__init__(stream, data.from_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549b517-bcce-4ef9-918b-3d4508cde41c",
   "metadata": {},
   "source": [
    "# 1. Twitter Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007304b-c0e4-4596-853e-53aaec0e85b8",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731dd06-880c-46db-9063-d226dd0bf47a",
   "metadata": {},
   "source": [
    "```# import tweepy```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a71d3f-d38d-4f30-92a8-8ee247d2fb54",
   "metadata": {},
   "source": [
    "Here is a client that will produce random fake Tweet data for this demonstration. Access to the actual API requires authorization dependent on a user with an active account. I have an account with [Elevated](https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-level) access that I personally requested from Twitter, but I figured it would not be a good idea to publicly post the auth tokens on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979d271-1c65-4f3d-9b75-513484ad61b7",
   "metadata": {},
   "source": [
    "This begins the first step in the system: the ingest of content from Twitter. In this case, data comes from the Twitter filtered stream endpoint via `POST /2/tweets/search/stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17cc050-808b-47a8-aa3e-b8d9a69aa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3209d286-f368-47c0-9fc4-9ff922f2ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TweepyTweet:\n",
    "    id: int\n",
    "    text: str\n",
    "    created_at: datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d07c7-c66b-44a3-be83-2722a3abe3fd",
   "metadata": {},
   "source": [
    "Tweet text content is just a random string of 15 words, so the sentiment results will not be useful, but the demonstrations will be perfectly workable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a649559-a179-4018-b611-88b110e81409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweepyClient(ABC):\n",
    "    \"\"\" Mock tweepy.StreamingClient \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        word_site = \"https://www.mit.edu/~ecprice/wordlist.10000\"\n",
    "        response = requests.get(word_site)\n",
    "        self.vocab = [b.decode(\"utf-8\") for b in response.content.splitlines()]\n",
    "\n",
    "    def start(self, num: int = 100):\n",
    "        t = Thread(target=self.generate, args=(num,))\n",
    "        t.start()\n",
    "        return t\n",
    "\n",
    "    def generate(self, num: int):\n",
    "        for i in range(num):\n",
    "            tweet = TweepyTweet(\n",
    "                id=random.randint(0, 10000),\n",
    "                text=\" \".join(random.sample(self.vocab, 15)),\n",
    "                created_at=datetime.utcnow(),\n",
    "            )\n",
    "            self.on_tweet(tweet)\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_tweet(self, tweet):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f803bf-7e09-4c13-926f-d83dc7eac587",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e22fa-0878-4fd3-b8f2-109358d816aa",
   "metadata": {},
   "source": [
    "Data is represented using one of my abstract data types. I also define a schema for the Kafka SchemaRegistry service so that it can properly understand the incoming and outgoing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd5f32-1d6c-4879-b365-15f3e7ac1355",
   "metadata": {},
   "source": [
    "Twitter data only needs 3 attributes for representing Tweets:\n",
    "\n",
    "* **task** - the Heartbeat tasking. This is the subject of the search. In our case, I was searching for Russia-Ukraine information.\n",
    "* **content** - the text content of the Tweet.\n",
    "* **time** - timestamp with only second precision. High precision is not really needed for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac283bf0-1fa1-4a37-9e33-78d9b08bb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(ADT):\n",
    "    def __init__(self, task: str, content: str, time: int):\n",
    "        self.task = task\n",
    "        self.content = content\n",
    "        self.time = time\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def schema(cls) -> str:\n",
    "        return \"\"\"\n",
    "        {\n",
    "            \"name\": \"tweet\",\n",
    "            \"type\": \"record\",\n",
    "            \"namespace\": \"heartbeat\",\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"time\",\n",
    "                    \"type\": {\n",
    "                        \"type\": \"int\", \n",
    "                        \"logicalType\": \"timestamp-millis\"\n",
    "                    }\n",
    "                },\n",
    "                {\"name\": \"content\", \"type\": \"string\"},\n",
    "                {\"name\": \"task\", \"type\": \"string\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc03d3-e68d-4109-a78e-055b5f1c8d72",
   "metadata": {},
   "source": [
    "## Tweepy - Kafka Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d13ce0-1314-4598-a5d4-d33eeb941aa6",
   "metadata": {},
   "source": [
    "With some of the mocking and setup out of the way, we can look at the services. The first part of the Heartbeat system involves ingesting the data from Twitter and producing it to the \"ingest\" topic in Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350865a-dc78-4cfb-8618-860a346f6edb",
   "metadata": {},
   "source": [
    "This simple service just connects to the Twitter stream endpoint, and as Tweets are received, pushes structured data to the topic. Data comes in from a Python wrapper for the Twitter API called `Tweepy`, which is mocked above. This library allowed for very simple access to the Twitter API: the work here was just a matter of connecting the stream of data coming from Twitter and Tweepy to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0862db48-55f4-4847-9c6d-8d0d92fffc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterIngest(TweepyClient):\n",
    "    def __init__(self, task: str, producer: Producer):\n",
    "        self.task = task\n",
    "        self.producer = producer\n",
    "        super().__init__()\n",
    "\n",
    "    # This will get called when new Tweet data comes in\n",
    "    def on_tweet(self, tweet):\n",
    "        # Make a payload and send it to the Kafka producer\n",
    "        payload = Tweet(self.task, tweet.text, tweet.created_at.timestamp())\n",
    "        self.producer.produce(key=tweet.id, value=payload)\n",
    "\n",
    "    def poll(self) -> None:\n",
    "        # Pull Tweets until we run out\n",
    "        empty = False\n",
    "        while not empty:\n",
    "            time.sleep(0.3)\n",
    "            empty = self.producer.poll() == 0\n",
    "        print(\">>> Exiting Twitter ingest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2155b-c960-4ef2-bd45-80759d8eafaa",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e339d7-6945-4c9a-be1c-ea3426715556",
   "metadata": {},
   "source": [
    "Make a new Kafka service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7d6b1a-abc5-43fe-857e-67afd3083c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mKAFKA \u001b[0mConnected to new Kafka broker\n"
     ]
    }
   ],
   "source": [
    "stream = KafkaStream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c619cca-6713-41da-b75f-63e3ffccae9a",
   "metadata": {},
   "source": [
    "Make a producer for moving data into Kafka and a service that will pull from Twitter and send to Kafka via this producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fcebf05-6215-4e8d-b251-6303ca9534de",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = Producer(\"ingest\", Tweet, stream)\n",
    "ingest = TwitterIngest(\"RU-UKR\", producer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6a213-4713-436f-922b-63f36d10cc87",
   "metadata": {},
   "source": [
    "Run the demo for a total of 5 Tweets. We should see 5 tweets get pushed to the Kafka topic \"ingest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68068544-233c-424f-b3dc-afc57e2d0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 6863 = {'task': 'RU-UKR', 'content': 'featured hardly office turtle lonely expense pillow easily creating fully kijiji bankruptcy balanced despite name', 'time': 1652077269.927678}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 7655 = {'task': 'RU-UKR', 'content': 'crude widescreen personal movies compaq ntsc thursday forces museum african believed findlaw profits playback charleston', 'time': 1652077269.927722}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 8013 = {'task': 'RU-UKR', 'content': 'sc hardwood suddenly specs filter losses exact attractive sleep aud doug faith auburn gmc accordance', 'time': 1652077269.927738}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 2303 = {'task': 'RU-UKR', 'content': 'guard effectiveness campbell administration housing shaw republican ada satisfied hygiene center ridge motherboard engaging thou', 'time': 1652077269.927753}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 1315 = {'task': 'RU-UKR', 'content': 'constantly logs phones whereas summary disorders facility concern waterproof exam rand totally sigma dosage winner', 'time': 1652077269.927766}\n",
      ">>> Exiting Twitter ingest\n"
     ]
    }
   ],
   "source": [
    "t = ingest.start(num=5)\n",
    "ingest.poll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8b6fc-2341-4f15-bccc-a4e26d6427d7",
   "metadata": {},
   "source": [
    "So now we have an incoming stream of Tweets with a very simple data schema. Next, we just have to process this data for sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32759110-ed9b-4b21-b739-b3cddb906361",
   "metadata": {},
   "source": [
    "# 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f70f4-f8bc-4a7d-82ed-1714c8d92b1a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d428e7a-2aff-4a19-9d43-7406fda63b07",
   "metadata": {},
   "source": [
    "As before, there will be some more data structures to deal with. This time, we will be using data storage for sentiment analysis results. This can be thought of as the system's method of making data \"models\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4faed-8f36-452d-9227-073f682a3de7",
   "metadata": {},
   "source": [
    "We are still storing the task information, but we are now also storing 3 different floating-point values for Tweet sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb214f8-282a-417f-a301-54fcd72b26ba",
   "metadata": {},
   "source": [
    "* **task** - the Heartbeat tasking. This is the subject of the search. In our case, I was searching for Russia-Ukraine information.\n",
    "* **time** - timestamp with only second precision. High precision is not really needed for this purpose. Should still be the timestamp from Twitter, not one that we create.\n",
    "* **pos** - likelihood of positive sentiment. Values range from 0 to 1 such that higher values indicate higher likelihood. Positive sentiment indicates favorable opinion of the tasking.\n",
    "* **neu** - likelihood of neutral sentiment. Neutral sentiment indicates no particular positive or negative opinion.\n",
    "* **neg** - likelihood of negative sentiment. Negative sentiment indicates some degree of dislike with the tasking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9fb7851-db24-4204-95d1-bbc52163fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment(ADT):\n",
    "    def __init__(\n",
    "        self, task: str, time: int, pos: float, neu: float, neg: float\n",
    "    ):\n",
    "        self.task = task\n",
    "        self.time = time\n",
    "        self.pos = pos\n",
    "        self.neu = neu\n",
    "        self.neg = neg\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def schema(cls) -> str:\n",
    "        return \"\"\"\n",
    "        {\n",
    "            \"name\": \"sentiment\",\n",
    "            \"type\": \"record\",\n",
    "            \"namespace\": \"heartbeat\",\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"time\",\n",
    "                    \"type\": {\"type\": \"int\", \"logicalType\": \"timestamp-millis\"}\n",
    "                },\n",
    "                {\"name\": \"task\", \"type\": \"string\"},\n",
    "                {\"name\": \"pos\", \"type\": \"float\"},\n",
    "                {\"name\": \"neu\", \"type\": \"float\"},\n",
    "                {\"name\": \"neg\", \"type\": \"float\"}\n",
    "            ]\n",
    "        }\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732c924-d77e-4dd0-a031-bd2d4fa29f6c",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa7c3b-3f50-469e-b58a-368f23b936cc",
   "metadata": {},
   "source": [
    "We will use a pre-trained model from [HuggingFace](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) for running our sentiment analysis. This model is a transformer model based on the Bidirectional Encoder Representations from Transformers (BERT) model from [Google](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html). This version was trained on 124 million Tweets from January 2018 to December 2018 and fine-tuned for the task of sentiment analysis. The research behind this came from [TimeLMs: Diachronic Language Models from Twitter](https://arxiv.org/pdf/2202.03829.pdf), where authors explored language model degradation and language shifts via social media."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23aaa0-6da3-4bcd-aa3b-28682015f7f7",
   "metadata": {},
   "source": [
    "Tokenization will come from the transformer model, and only simple preprocessing will be done to handle links and usernames. The rest can be handled by the tokenizer itself since we do not want to lose any context that might be helpful for the transformer and the sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "109f2cbc-69dc-4dee-b4d3-ba2730655935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jklasa/anaconda3/envs/hbtest/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Load tokenizer, model, and config\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "\n",
    "    def score(self, text: str) -> List[float]:\n",
    "        # Assess sentiment of text\n",
    "        processed = SentimentAnalyzer.preprocess(text)\n",
    "        encoded_input = self.tokenizer(processed, return_tensors=\"pt\")\n",
    "        output = self.model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        return softmax(scores) # negative, neutral, positive\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text: str) -> str:\n",
    "        # Preprocess text (username and link placeholders)\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = \"@user\" if t.startswith(\"@\") and len(t) > 1 else t\n",
    "            t = \"http\" if t.startswith(\"http\") else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364ed390-8f34-4ebc-ac97-bba3e34e043b",
   "metadata": {},
   "source": [
    "Below is a simple routine to analyze Tweets. We consume data from the Kafka topic \"ingest\", run model inference, and finally push the data to a different Kafka topic \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa948757-817a-495d-8019-e334893ea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(stream, analyzer):\n",
    "    consumer = Consumer(Tweet, stream)\n",
    "    consumer.subscribe(\"ingest\")\n",
    "    producer = Producer(\"sentiment\", Sentiment, stream)\n",
    "\n",
    "    empty = 0\n",
    "    while empty < 5:\n",
    "        msg = consumer.poll()\n",
    "        if msg is None:\n",
    "            empty += 1\n",
    "            continue\n",
    "\n",
    "        key, tweet = msg\n",
    "        if tweet is not None:\n",
    "            negative, neutral, positive = analyzer.score(tweet.content)\n",
    "            sentiment = Sentiment(\n",
    "                task=tweet.task,\n",
    "                time=tweet.time,\n",
    "                pos=positive,\n",
    "                neu=neutral,\n",
    "                neg=negative,\n",
    "            )\n",
    "            producer.produce(key=key, value=sentiment)\n",
    "        producer.poll()\n",
    "    print(\">>> Exiting sentiment analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d393bb-389b-4b2b-842e-bd97291451d8",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee2f98-ff29-4f22-8eaf-50eafff238cc",
   "metadata": {},
   "source": [
    "Let's see how some of this would work. First, we just have to load the model from HuggingFace, where pretrained models are made extremely accessible for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1d8d465-346e-4aa8-b054-7b718fc9cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Set up sentiment analysis\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22101fc3-7352-49b5-a220-1bd067549fca",
   "metadata": {},
   "source": [
    "With the models loaded, we can determine sentiment from the text. Note that the return order from the model is [negative, neutral, positive]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb302259-d23d-40f6-92e8-eca47cd7a225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00420294, 0.00935337, 0.98644376], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.score(\"We are absolutely loving this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13e3db83-b29e-47d1-b8a9-0f7d4714ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8545521 , 0.11869626, 0.02675165], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.score(\"This is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93417e7c-34ce-4d17-8d4a-70c5b5d3a600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02685222, 0.7657344 , 0.20741342], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.score(\"HuggingFace is a machine learning library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ab62e-3165-4e4a-ba6d-3b0342bcb0f4",
   "metadata": {},
   "source": [
    "Looking at the sentiment results, we can see they make sense: the first example is positive, the second one is negative, and the third one is neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e61c1e-dc14-4bc5-820a-8d34630d96f0",
   "metadata": {},
   "source": [
    "We can even attach this analyzer to the Tweet data. The text is random gibberish, so the sentiment outputs are not very useful, but the system can still be demonstrated in this manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf6b5dd6-c979-4ca7-8760-21fd926e1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoAnalyzer(TweepyClient):\n",
    "    def __init__(self, analyzer, *args, **kwargs):\n",
    "        self.analyzer = analyzer\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def on_tweet(self, tweet):\n",
    "        # Called when new Tweets come in\n",
    "        print(f\"> TEXT: {tweet.text}\")\n",
    "        pos, neu, neg = self.analyzer.score(tweet.text)\n",
    "        print(f\"  SENTIMENT: pos={pos:.3f}, neu={neu:.3f}, neg={neg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b325b042-9c03-43b6-92fb-61ddfa858ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DemoAnalyzer(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15e11bfb-4348-40d1-b88c-813eace8a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> TEXT: merger issued listings believe palm eagle cayman ringtones posters britannica traditions laws witness exams league\n",
      "  SENTIMENT: pos=0.020, neu=0.930, neg=0.050\n",
      "> TEXT: hell faced deluxe src revolutionary infection small georgia functional sn withdrawal satisfactory particularly macedonia pichunter\n",
      "  SENTIMENT: pos=0.164, neu=0.735, neg=0.101\n",
      "> TEXT: wearing partnerships em pop approaches improvement custom eleven concluded stay eva harvard jr lips ws\n",
      "  SENTIMENT: pos=0.011, neu=0.936, neg=0.053\n",
      "> TEXT: epinionscom au petition forecasts ol bond climb chan hayes worry fd maria registrar mass pod\n",
      "  SENTIMENT: pos=0.081, neu=0.868, neg=0.051\n",
      "> TEXT: astronomy particles responsibility bargains interests cam southeast punch bugs showers breaks specify convinced machines retired\n",
      "  SENTIMENT: pos=0.141, neu=0.824, neg=0.035\n"
     ]
    }
   ],
   "source": [
    "y = da.start(5)\n",
    "y.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f363f1b-0d42-4ed6-b921-5d646ad220e5",
   "metadata": {},
   "source": [
    "# 3. Database Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277d154-96de-486a-ae16-71f72b5bc70a",
   "metadata": {},
   "source": [
    "The last step is transfer of the results from Kafka to a database for storage. For this project, I used [InfluxDB](https://github.com/influxdata/influxdb), an efficient time-series database for storing the sentiments according to the timestamps at which they were retrieved. This will again be mocked with a simple write and get_all interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca507c79-0740-4774-b100-11b9d99b1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self, lock):\n",
    "        self.data = []\n",
    "        self.lock = lock\n",
    "        \n",
    "    def write(self, item):\n",
    "        self.lock.acquire()\n",
    "        print(fg.grey + \"DB \" + rs.all, end=\"\")\n",
    "        print(fg.green + \"ADD\" + rs.all + f\" {item}\")\n",
    "        \n",
    "        self.lock.release()\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def get_all(self):\n",
    "        print(\"Database dump...\")\n",
    "        for idx, item in enumerate(self.data):\n",
    "            print(f\"{idx}) {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9049b-0b51-4336-95a5-e2f9ad9c15a8",
   "metadata": {},
   "source": [
    "Below is a routine to consume data from Kafka topic \"sentiment\" and push to the database for the final destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "507ba7ac-7363-42dc-9efd-f8dbd1cf5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(stream: KafkaStream, db: Database, size: int):\n",
    "    consumer = Consumer(Sentiment, stream)\n",
    "    consumer.subscribe(\"sentiment\")\n",
    "\n",
    "    count = 0\n",
    "    while count < size:\n",
    "        msg = consumer.poll()\n",
    "        if msg is None:\n",
    "            continue\n",
    "\n",
    "        count += 1\n",
    "        key, sent = msg\n",
    "        db.write([key, sent.task, sent.time, sent.pos, sent.neu, sent.neg])\n",
    "    print(\">>> Exiting data storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa83566-1b80-475f-98d9-35451ad3eecb",
   "metadata": {},
   "source": [
    "# Full Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9ff13-ce61-4cc5-a09c-403a8b423210",
   "metadata": {},
   "source": [
    "Now let's put it all together and let the ingest service retrieve data from \"Twitter\" and push it to \"Kafka\". Meanwhile, we will let the analyzer service retrieve data from \"Kafka\" and produce sentiment analysis results. Finally, these results will be pushed to our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85a91d5b-98eb-43cb-9896-685118a857bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mKAFKA \u001b[0mConnected to new Kafka broker\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 451 = {'task': 'RU-UKR', 'content': 'israel something aware nano drop imagination synthetic wireless kay cartridges few engage industry gathered war', 'time': 1652077277.386532}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 1049 = {'task': 'RU-UKR', 'content': 'herbal tool reflect collapse interpretation looks tables burton bean mitsubishi frozen wars belfast cohen assistant', 'time': 1652077277.386616}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 6458 = {'task': 'RU-UKR', 'content': 'improvement expense shoes formal specific summer love save samba carroll algorithms diseases hungarian date registry', 'time': 1652077277.386634}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 6163 = {'task': 'RU-UKR', 'content': 'raleigh flooring leaf pdas johnston showed ecology aspect elimination class res ultra eight releases of', 'time': 1652077277.38666}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[32mADD\u001b[0m 2627 = {'task': 'RU-UKR', 'content': 'pro revision job testimony interior experiments consent swedish afghanistan diego format polished revelation grand on', 'time': 1652077277.386681}\n",
      ">>> Exiting Twitter ingest\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 451 = {'task': 'RU-UKR', 'content': 'israel something aware nano drop imagination synthetic wireless kay cartridges few engage industry gathered war', 'time': 1652077277.386532}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 451 = {'task': 'RU-UKR', 'time': 1652077277.386532, 'pos': 0.0462605, 'neu': 0.81405675, 'neg': 0.13968267}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 451 = {'task': 'RU-UKR', 'time': 1652077277.386532, 'pos': 0.0462605, 'neu': 0.81405675, 'neg': 0.13968267}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 1049 = {'task': 'RU-UKR', 'content': 'herbal tool reflect collapse interpretation looks tables burton bean mitsubishi frozen wars belfast cohen assistant', 'time': 1652077277.386616}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [451, 'RU-UKR', 1652077277.386532, 0.0462605, 0.81405675, 0.13968267]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 1049 = {'task': 'RU-UKR', 'time': 1652077277.386616, 'pos': 0.03076393, 'neu': 0.8217568, 'neg': 0.1474793}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 1049 = {'task': 'RU-UKR', 'time': 1652077277.386616, 'pos': 0.03076393, 'neu': 0.8217568, 'neg': 0.1474793}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 6458 = {'task': 'RU-UKR', 'content': 'improvement expense shoes formal specific summer love save samba carroll algorithms diseases hungarian date registry', 'time': 1652077277.386634}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [1049, 'RU-UKR', 1652077277.386616, 0.03076393, 0.8217568, 0.1474793]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 6458 = {'task': 'RU-UKR', 'time': 1652077277.386634, 'pos': 0.06477577, 'neu': 0.8149433, 'neg': 0.120280795}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 6458 = {'task': 'RU-UKR', 'time': 1652077277.386634, 'pos': 0.06477577, 'neu': 0.8149433, 'neg': 0.120280795}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 6163 = {'task': 'RU-UKR', 'content': 'raleigh flooring leaf pdas johnston showed ecology aspect elimination class res ultra eight releases of', 'time': 1652077277.38666}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [6458, 'RU-UKR', 1652077277.386634, 0.06477577, 0.8149433, 0.120280795]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 6163 = {'task': 'RU-UKR', 'time': 1652077277.38666, 'pos': 0.04155181, 'neu': 0.9429347, 'neg': 0.015513566}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 6163 = {'task': 'RU-UKR', 'time': 1652077277.38666, 'pos': 0.04155181, 'neu': 0.9429347, 'neg': 0.015513566}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m 2627 = {'task': 'RU-UKR', 'content': 'pro revision job testimony interior experiments consent swedish afghanistan diego format polished revelation grand on', 'time': 1652077277.386681}\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [6163, 'RU-UKR', 1652077277.38666, 0.04155181, 0.9429347, 0.015513566]\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[32mADD\u001b[0m 2627 = {'task': 'RU-UKR', 'time': 1652077277.386681, 'pos': 0.12736656, 'neu': 0.8120202, 'neg': 0.060613222}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35msentiment \u001b[0m\u001b[31mGET\u001b[0m 2627 = {'task': 'RU-UKR', 'time': 1652077277.386681, 'pos': 0.12736656, 'neu': 0.8120202, 'neg': 0.060613222}\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[38;5;249mDB \u001b[0m\u001b[32mADD\u001b[0m [2627, 'RU-UKR', 1652077277.386681, 0.12736656, 0.8120202, 0.060613222]\n",
      ">>> Exiting data storage\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      "\u001b[36mKAFKA \u001b[0m\u001b[35mingest \u001b[0m\u001b[31mGET\u001b[0m EMPTY\n",
      ">>> Exiting sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "# Set up the stream\n",
    "stream = KafkaStream()\n",
    "\n",
    "# Set up the database\n",
    "db = Database(stream.lock)\n",
    "\n",
    "# 1. Twitter Ingest\n",
    "producer = Producer(\"ingest\", Tweet, stream)\n",
    "ingest = TwitterIngest(\"RU-UKR\", producer)\n",
    "t1 = ingest.start(5)\n",
    "t2 = Thread(target=ingest.poll())\n",
    "\n",
    "# 2. Sentiment Analysis\n",
    "t3 = Thread(target=analyze, args=(stream, analyzer))\n",
    "\n",
    "# 3. Database Storage\n",
    "t4 = Thread(target=transfer, args=(stream, db, 5))\n",
    "\n",
    "threads = [t1, t2, t3, t4]\n",
    "for t in threads[1:]:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff55229-1937-4cf3-940c-c6e12f0b241d",
   "metadata": {},
   "source": [
    "And then, of course, we can get the results in the database and easily inspect them. InfluxDB makes this very easy as well, as it has built-in dashboards for visualization of time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaea306e-62e2-4887-92b5-293fb13534cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database dump...\n",
      "0) [451, 'RU-UKR', 1652077277.386532, 0.0462605, 0.81405675, 0.13968267]\n",
      "1) [1049, 'RU-UKR', 1652077277.386616, 0.03076393, 0.8217568, 0.1474793]\n",
      "2) [6458, 'RU-UKR', 1652077277.386634, 0.06477577, 0.8149433, 0.120280795]\n",
      "3) [6163, 'RU-UKR', 1652077277.38666, 0.04155181, 0.9429347, 0.015513566]\n",
      "4) [2627, 'RU-UKR', 1652077277.386681, 0.12736656, 0.8120202, 0.060613222]\n"
     ]
    }
   ],
   "source": [
    "db.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73381f-ad3b-419c-952d-18543a0aab86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
